
# Data-Centric Systems 

I'm interested in developing data-centric AI systems. Nearly every industry has figured out how to vertically align critical knowledge bases that can positively impact decision making. Aligning what's possible with computational enviornments, engineering talent, and capital investment restrictions requires a re-vamped mental framework to consider. Information that forumlates knowledge bases often serve different end-purposes, with an everchanging level of abstraction and dimensional analysis to create value. For example, the average file cabinet is a knowledge base of information, but is a substationally outdated method in interacting with information. It's an investment to digitally store information securley and neatly, and while storage cost may not be a factor, the cost required to extract value may change the value of the information. Much like phyiscal file cabinets, Data-Centric AI systems serve similar objectives that enable people to interact with information quickly, ethically, and usefully. Thus, information and the raw data that generates information should be treated as an asset class, epsecially when considered to be used as a collective part of artifical intelligence (AI) generation. This is especially important for startups, research-oriented investments, or any zero-to-one building experience.  

Through experience, I've learned that like any other organizational asset, data has both a cost and a value that must be carefully considered. As an asset class, data can be valuable when trying to gain a strategic advantage and could always be considered as a *potential* advantage when collected organically. However, data can also be used incorrectly and consequentially overvalued as a strategic asset; therefore, measuring the value of information is where I start for any data-centric AI development process. While data should be treated as another type of organizational asset, the process(s) of data collection, maintenance, and implementation required to unlock value could turn the raw material (data) into a liability. Thus, when measuring the value of the intangible asset - information, DevOps and experimentation teams should also measure the value of the process to data implementation. Assessing the required investment to pre-process data, refine data quality, store, or implement should be weighed as an added cost for any data asset, especially when driving product UX. 

# Assesing the Cost and Value of Information 

Without referencing a specific data type, some characteristics reflect the similarity of data, the asset, to conventional asset classes. I think this exercise should be systematically iterated within any business, for it initiates consistent practice in understanding the true value being delivered to an end-user, rather than reporting it as just an IT resource. Additionally, while data can be an asset, it almost always starts an investment for the immediate generated value isn't always clear, known, or even fully understood whether it's a product, system design, marketplace, feature, or internal tool. So, as development teams pursue developing data beyond its bleak, standardized formats - information should be held to unique standards of *accounting* that are similar to the formal accounting procedures of conventional assets. 

When considering its valuation, there is an abundance of methodologies to choose from as new software and hardware options could be utilized, depending on scenarios, and impact the total required "investment." In an ideal scenario, organizations would not only measure the cost and value of their information assets but also recognize the multiplicity of factors influencing their worth. Just as buildings and machines have diverse valuation methods, information assets may require a spectrum of valuation techniques considering factors like diversity of data sources, data quality, and relevance to different stakeholders.

Envision an ideal scenario where organizations can accurately access both the cost and the value of their information assets. In this scenario, these information assets would be integrated into the balance sheet alongside tangible assets such as buildings and machinery, as well as financial assets like cash and receivables. This integration would represent the pinnacle of accountability. When information assets treated as strategic assets, spontaneous concerns regarding their efficient investment and management would diminish. They would be governed by the same financial disciplines that ensure resources are utilized effectively in a properly managed business. Strategies such as information management maturity models have been used to navigate organizations by benchmarking their current strategy, targeting future performance metrics, simplifying frameworks, and measuring value beyond an existing service offering. 

There are many approaches used to measure the value of information. Daniel Moody, from the University of Melbourne, recognizes some key principles/laws in his "Measuring The Value of Information: An Asset Valuation Approach" that follow accounting and strategic management principles. Chris Higson and Dave Waltho scripted "Valuing Information as an Asset" and argue "the need for an asset-centric, value-based approach to the management of information, as opposed to the still common, but often unsuccessful, security-centric approach that tends to view information as a toxic liability." In addition to yielding a positive sentiment on the core characteristics of organic data channels, selecting an applicable framework for valuation may require several attempts with *exquisite assumptions and parameter selection.* Here's an example of a qualitative "first-take" of defining the characteristics of strategic assets. 

# Example Checklist for Defining Characteristics of a Strategic Asset
**as interpreted from Daniel Moody's "Measuring the Value of Information: Asset Valuation Approach"**

**Does data service have potential or future economic benefits?**

- An asset must be expected to provide future services or economic benefits, either through its use (insight visualization, communication, etc) or sale (transaction with another organization).
  
**Does data give an organization more strategic ability?**

- The organization must have the capacity to benefit from the asset and regulate access to it by others. When an organization possesses information, it can control access to and use of that asset, denying or regulating others' access.
  
**How has the data been collected? Is it organic from origination to storage based on previous transactions?**

- Control over the asset must have been obtained through past purchases, internal development, or discovery. Information is typically collected as a byproduct of internal operations, where the data from the past could be "re-enforced" inside decision-making processes for the next decision or investment. Transactions can be continuous or event "on-event," but the nature by which data appears to an organization can create more defensibility, depending on accessibility, proprietary foundations of generating the data in the first place, and privacy regulations. 



# Driving Product towards Value Propositions:

My product development interests lie at the intersection of data-driven methods, machine learning, and optimization techniques to uncover actionable insights from complex datasets. My goal is to empower end-users (Developers, APIs, Customers, and Members) to make informed decisions by effectively communicating data-driven insights with transparency, accuracy, and continuous optimization. When crafting user experiences (UXs) of data-centric insight mechanisms, it's important to consider the objectives of the decision-making process within organizations. Thus, modernizing digital work fronts should offer teams & individual contributors with customization, personalization, interaction, and/or discoverability pathways. By prioritizing data-centric modeling and user-centric design principles, my objective is to give users insights that inform decision-making, including interaction with the information. This entails crafting intuitive user interfaces, refining data processing pipelines, and iteratively enhancing algorithms to optimize the overall user experience.


# Addressable Questions: "Data-Driven Applied Optimization" 
When embarking on a new project, I aim to consider the following questions:

- What are the key objectives and outcomes we aim to achieve?
- How can we leverage data-driven methods to drive actionable insights?
- What are the most effective ways to engage and empower end-users with generated intelligence?
- How can we create reinforcement learning mechanisms within applications to streamline continuous optimization?
- How can we leverage generative AI, LLMs, and foundational models to bring human interaction to the next level? 
- How can we integrate universal constants & uncertainty quantification into prediction mechanisms? This includes partial-differential equations, physics-based constraints, and mathematical symmetries.
- How can we improve continuous optimization with programmatic differentiation, where parameterized blocks, automatic differentiation, and optimization capabilities adapt to new, real-time conditions?
- How can end-users, customers, viewers, and anyone deriving value communicate & interact best with the data & insights needed to make a decision?
- If client-server architectures are "stateless," with no memory storage, what does a new request-response model look like? Do augmented memory storage responses aid development?
- Can the implementation of retrieval-augmented processes help make real-time conditions easier to decipher?
- Does a new request-response improve outcomes & decision-making of existing model performance?
- What's the "multi-player" mode of an existing software tech stack that can maximize individual decision-making outcomes? 


## Current Project Interest: Data-Driven Control, Optimization, & Observability Strategies  

**Systems Modeling & Observability of Complex Systems**

There's great value in integrating machine learning, engineering mathematics, and mathematical physics to model, predict, & control dynamic systems for end-users to act faster. With interactive, personalized, or customized insight generation systems, time wasted on manual efforts is replaced by more creative and discoverable user experiences. In addition to maintaining clean, computationally-effective data pipelines, teams should consider data-driven methodologies that enhance predictive accuracy and understanding of non-linear behaviors as systems interact with new environments & conditions. 

This may involve exploring different computational architectures, such as software-hardware integration, client-server models, GPU-sensor integration, or peer-to-peer networks, to determine the most effective approaches for extracting valuable information. By developing a deeper understanding of how to interpret latent representations within the data, models & SaaS solutions can adapt to the conditional requirements, for each task the data correlates to. 
 
To effectively simplify the extraction of patterns, features, parameters, or raw data derived from non-linear environments, leveraging machine learning techniques to identify and model complex non-linear systems with high-frequency time-series data generation is imperative. Decision-making experiences, with the collaboration of data-driven transformations, enable users to systemically interact with data beyond conventional mediums where uncertainty levels in outcomes remain unaffected. Bringing observability through the abstractions of value-creation opportunities in real-time directs individual contributors toward optimal strategies faster. Highly successful parameter selection, feature extraction, data/coordinate transformation, memory compression, or retrieval-augmented generation process will lead to smarter decision-making time. 

**Data-Driven Discovery**

In data-rich environments, exploring advanced methods for modeling, prediction, and control of complex systems is increasingly crucial. Leveraging cutting-edge techniques to simplify models through pre-trained transformations with extensive contextual knowledge is key to achieving accurate outputs that guide end-users effectively. Data-driven discovery processes such as reduced order models, enable clean interpolation of outputs and raw data. Employees stuck with large-scale siloed solutions with an abundance of frequency, type, and storage channels for data collection wrangle messy data together and apart for weeks to hours. Reduce Order Models (ROMs) correspond to reduced-order bases (ROBs) and typically belong to nonlinear, matrix manifolds, and classical interpolation methods fail to enforce the constraints characterizing those manifolds. Sometimes flat, constraint-free space transformations that interpolate the parametric data using a conventional approximation method, and can successfully transport the "result" back to the originating space. While this is complicated, non-technical operators with deep experience in an organization's decision-making strategy can impact system designs with AI objectives akin to existing processes. 

ABACUS.AI - a San Francisco startup that refers to their technology as the " world’s first AI platform where AI, not humans, build Applied AI agents and systems at scale" wrangles data and deploys scalable models for developers utilizing rapid transformations of data. From linear, to cosine, to 3D Hermite and beyond - reduced-order models (ROMs) can be adapted and interpolated through various methods. 

Here are some key elements of an effective data-driven discovery process that I am focusing on: 

- *Dimensional Analysis and Feature Selection:* Uncovering patterns through dimensional analysis, and selecting relevant features using probabilistic methods, such as SHAP, to streamline the modeling process. 
- *Deep-Knowledge Spaces and Memory Compression:* By creating deep-knowledge spaces, employing memory compression techniques, guided through mathematical and physical principles, the accuracy of outputs is significantly enhanced. 
- *Reduced Order Models (ROMs) for Interpolation:* Utilizing reduced order models facilitates seamless interpolation of outputs between humans and raw data, enabling effective communication and understanding of complex datasets. 

**Differential Programming** 

Differentiable Programming (DP) is a technique that enables the automatic computation of derivatives of model outputs concerning model parameters. Pathmind describes DP as "programs that rewrite themselves at least one component by optimizing along a gradient, like neural networks. " This approach allows for the potential to build end-to-end differentiation of complex computer programs, including those with control flows and data structures, facilitating gradient-based optimization of program parameters. Automatic differentiability is an essential ingredient in the construction of such hybrid models, where generalized models are parsed with elements that exist externally in a back-end repository. Parameter blocks are utilized to add structural parameters to a model, enabling the incorporation of additional parameters and defining relationships between different components of the model. DP techniques create the unique opportunity to implement adaptive control techniques to handle nonlinear dynamics of physical & non-physical systems, where direct relationships are not easy to forecast and model. Due to the complexity, cost, and intricacies of behaviors within large-scale systems, using nonlinear dynamics to capture behaviors accurately is hard! Granting vertically aligned observability over the structure, functions, algorithmic selection, or parameter classification, of repository structures can offer organizations an opportunity to drive data governance through an organization's managerial systems. 

**Augmented Memory** 

When augmenting memory storage architectures to enhance real-time conditions in client-server architectures makes sense to apply, teams have an opportunity to decipher data faster without compromising scalability and reliability. While traditional client-server architectures are stateless, incorporating memory storage in systems can improve the request-response model. By utilizing memory storage, systems can retain critical information, making it easier to decipher real-time conditions and respond more efficiently to user requests.
