
# Data-Centric Sytems 

I'm interested in developing data-centric AI systems. As an asset class, data can be valuable when trying to gain a strategic advantage and should always be considered as a *potential* advantage when collected organically. However, data can also be used incorrectly and consequentially overvalued as an asset; therefore, measuring the value of information is where I tend to start for any data-centric AI development process. 

I strongly believe that information, like any other organizational asset, has both a cost and a value that must be carefully considered. While data should be treated as another type of organizational asset, the process(s) of data collection, maintenance, and implementation required to unlock value could turn the raw material (data) into a liability. Thus, when measuring the value of the intangible asset - information - one should also measure the value of the process to data implementation. I think the connection of raw data with the mechanisms, people, and computers that interact with the information uniquely create another opportunity of data utilisation. With that being said, assesing the required investment to pre-process data, refine data quality, or perform computationally intesive tasks, should be weighed as an added cost for any data asset. 

# The Dual Nature of Information: Cost and Value

At a high level, without referencing a specfic data-type, we can use a few charactersitcs to compare data to conventional assets classes. I think this simple exercise should be systematically iterated within an any business, for it initiates the process of recognizing data as an asset, rather than just an IT resource. As an asset, information should be held to unique standards of accountability, but when considering its valuation, we must also acknowledge the diversity of methodologies involved. In an ideal scenario, organizations would not only measure the cost and value of their information assets but also recognize the multiplicity of factors influencing their worth. Just as buildings and machines have diverse valuation methods, information assets may require a spectrum of valuation techniques considering factors like diversity of data sources, data quality, and relevance to different stakeholders. 

Incorporating this perspective into the gold standard of "accountability" would necessitate a nuanced approach that respects the complexity of information valuation in today's conventional high-frequency data generation landscape. Envision an ideal scenario where organizations can accurately assess both the cost and the value of their information assets. In this scenario, these information assets would be integrated into the balance sheet alongside tangible assets such as buildings and machinery, as well as financial assets like cash and receivables. This integration would represent the pinnacle of accountability. With information assets treated in this manner, concerns regarding their efficient investment and management would diminish. They would be governed by the same financial disciplines that ensure resources are utilized effectively in a properly managed business.

While accounting regulations currently exclude 'intangible' assets such as information from the balance sheet, despite their often being the primary driver of business value. As advanced economies transition from the industrial era to the information age to the intelligence age, the significance of various asset categories has shifted, with intangibles now taking precedence as the foremost asset class. 

There are many approaches used to measure the value of information. Daniel Moody, from the University of Melbourne, recongizes some key principles in his "Measuring The Value of Information: An Asset Valuation Approach" that follow accounting and strategic management principles. Chris Higson and Dave Waltho scripted "Valuing Information as an Asset" and argue "the need for an asset-centric, value-based approach to the management of information, as opposed to the still common, but often unsuccessful, security-centric approach that tends to view information as a toxic liability." Thus, in addition to core characterstics selecting an applicable framework for valuation may require multiple attempts. 

# Example Checklist for Defining Characteristics of a Strategic Asset
**as written by Daniel Moody "Measuring the Value of Information: Asset Valuation Approach"**

1) Does "Data" Service Potential or Future Economic Benefits ? 

- An asset must be expected to provide future services or economic benefits, either through its use or sale. For the projects I take on, I focus on the utilization of data - either intriscally through products system archtecture or in communication to an end-user.  

2) Does "Data" grand an organization more power?

- The organization must have the capacity to benefit from the asset and regulate access to it by others. When an organization possesses information, it can control access to and use of that asset, denying or regulating others' access.
  
3) How has the Data be collected? Is it organic from origination to storage (based on previous transactions?)

- Control over the asset must have been obtained through past purchases, internal development, or discovery. Information is typically collected as a byproduct of internal operations, purchased, or discovered through analysis, representing past investments. Transactions can be continous or "on-event," but the nature by which data appears to an organization can create more defensibility as it is used.


# Product Interest
My product development interest lie at the intersection of data-driven methods, machine learning, and optimization techniques to uncover actionable insights from complex datasets. My goal is to empower end-users to make informed decisions by effectively communicating data-driven insights with transparency, accuracy, and continuous optimization. When crafting user-experiences (UXs) of data-centric insight mechanisms, it's important to consider the objectives of the decision-making process within organizations. Thus, modernizing digital workfronts should offer teams & individual contributors with customization, personalization, interaction, and/or  discoverbilitiy pathways. By prioritizing data-centric modeling and user-centric design principles, my objective is to empower users with insights that catalyze informed decision-making, including interaction the  information. This entails crafting intuitive user interfaces, refining data processing pipelines, and iteratively enhancing algorithms to optimize the overall user experience.


# Addressable Questions : "Data Driven Applied Optimization" 
When embarking on a new project, I aim to consider the following questions:

- What are the key objectives and outcomes we aim to achieve?
- How can we leverage data-driven methods to drive actionable insights?
- What are the most effective ways to engage and empower end-users with generated intelligence?
- How can we create reinforcement learning mechanisms within applications to streamline continuous optimization?
- How can we leverage generative AI, LLMs, and foundational models to bring human-interaction to the next level? 
- How can we integrate universal constants & uncertainty quantification into prediction mechanisms. This includes partial-differential equations, physics-based constraints, and mathematical symmetries?
- How can we improve continuous optimization with programmatic differentiation, where parameterized blocks, automatic differentiation, optimization capabilities adapt to new, real-time conditions?
- How can end-users, customers, viewers, and anyone deriving value communicate & interact best with the data & insights needed to make a decision?
- If client-server archectectures are "stateless," with no memory storage, what does a new request-response model look like? Do augmented memory storage responses aid development?
- Can implementation of retrievel augemented processces help make real-time conditions easier to dechipher?
- Does a new request-response improve outcomes & decision making of exisitng model performance?
- What's the "multi-player" mode of an exisiting software tech-stack that can maximizes individual decision making outcomes? 


## Current Project Interest: Data-Driven Control & Observability Strategies  

**Systems Modeling & Observability of Complex Systems**

There's great value in intergrating machine learning, engineering mathematics, and mathematical physics to model, predict, & control dynamic systems for end-users to act faster. With interactive, personalized, or customized insight generation systems, time wasted on manual efforts is replaced by more creative and discoverable user-experiences. In addition to maintaining clean, computationally-effective data pipelines, I seek to bring data-driven methodologies to enhance predictive accuracy and understandings of non-linear behaviors as systems interact with new environments & conditions. 

My goal is to leverage advanced signal processing techniques to uncover meaningful insights from complex, dynamic data. This may involve exploring different computational architectures, such as software-hardware integration, client-server models, GPU-sensor integration, or peer-to-peer networks, to determine the most effective approaches for extracting valuable information. By developing a deeper understanding of how to interpret latent representations within the data, I hope to create innovative solutions that can adapt to the unique requirements of each analysis task. 
 
In order to effectivly simplify the extraction of patterns, features, parameters, or raw data dervived from non-linear environements, leveraging machine learning techniques to identify and model complex dynamic systems is imperitive. Decision making experiences, with collaboration of data-driven transformations, enable users to systemically interact with data beyond conventional mediums where uncertaity levels in outcomes remain unaffected. Bringing observability through the abstractions of value-creation opportunities in real-time directs invidual contributors towards optimal strategies faster. Highly-succesful parameter selection, feature extraction, data/coordinate transformation, memory compression, or retrieval-augmented generation process will lead to smarter decision making overtime. 

**Data-Driven Discovery**

In data-rich environments, exploring advanced methods for modeling, prediction, and control of complex systems is increasingly crucial. Leveraging cutting-edge techniques to simplify models through pre-trained transformations with extensive contextual knowledge is key to achieving accurate outputs that guide end-users effectively. Data-driven discovery processes such as reduced order models, enable clean interoplation of outputs and raw data. Employees stuck with large-scale siloed solutions with an abudance in frequecy, type, and storage channels for data collection wrangle messy data togther and apart for weeks-to-hours. ROMs correspond to reduced-order bases (ROBs) and typically belong to nonlinear, matrix manifolds, and classical interpolation methods fail to enforce the constraints characterizing those manifolds. Sometimes flat, constraint-free space transformations that interpolates the parametric data using a conventional approximation method, and can succesfully transport the "result" back to the originating space. ABACUS.AI - a San Francisco startup that refers to their technology as " worldâ€™s first AI platform where AI, not humans, build Applied AI agents and systems at scale" wrangles data and deploys on behalf of developers utilizing rapid transformations of data. From linear, to cosine, to 3D hermite and beyond - reduced-order models (ROMs) can be adapted and interpolated through various methods. 

Here are some key elements of an effective data-driven discovery process that I am focusing on: 

- *Dimensional Analysis and Feature Selection:* Uncovering patterns through dimensional analysis, and selecting relevant features using probabilistic methods, such as SHAP, to streamline the modeling process. 
- *Deep-Knowledge Spaces and Memory Compression:* By creating deep-knowledge spaces, employing memory compression techniques, guided through mathematical and physical principles, accuracy of outputs is significantly enhanced. 
- *Reduced Order Models (ROMs) for Interpolation:* Utilizing reduced order models facilitates seamless interpolation of outputs between humans and raw data, enabling effective communication and understanding of complex datasets. 

**Differential Programming** 

Differentiable Programming (DP) is a technique that enables the automatic computation of derivatives of model outputs with respect to model parameters. Pathmind describes DP as "programs that rewrite themselves at least one component by optimizing along a gradient, like neural networks. "  This approach allows for the potential to build end-to-end differentiation of complex computer programs, including those with control flows and data structures, facilitating gradient-based optimization of program parameters. Automatic differentiability is an essential ingredient in the construction of such hybrid models, where generalized models are parsed with elements that exist externally to a back-end repository. Parameter blocks are utilized to add structural parameters to a model, enabling the incorporation of additional parameters and defining relationships between different components of the model. DP techniques create the unqiue opportunity to implement adaptive control techniques to handle nonlinear dynamics of physical & non-physical systems, where direct relationships are not easy to forecast and model. Due to the complexity, cost, and intricacies of behaviours within large-scale systems, using nonlinear dynamics to capture behaviors accurately is hard! Granting vertically aligned observabiltity over the structure, functions, algorithmic selection, or parameter classification, of repository structures can offer organizations an opportunity to drive data governance through an organizations managerial systems. 

**Augmented Memory** 

When augmenting memory storage architectures to enhance real-time conditions in client-server architectures makes senses to apply, teams have an opportunity to dechiper data faster without compropmising scalability and reliability. While traditional client-server architectures are stateless, incorporating memory storage in systems can improve the request-response model. By utilizing memory storage, systems can retain critical information, making it easier to decipher real-time conditions and respond more efficiently to user requests.
